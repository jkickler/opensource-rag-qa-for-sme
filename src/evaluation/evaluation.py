import datetime
import json
import random

import pandas as pd
from evaluation.eval_templates import (
    qa_generation_prompt,
    qa_response_schema,
    bing_chat_template,
    bing_chat_response_schema,
)
from langchain.output_parsers import StructuredOutputParser
from langchain.prompts import PromptTemplate
from langchain_community.llms import LlamaCpp
from langchain_community.embeddings import HuggingFaceEmbeddings
from tqdm import tqdm
from utils.logging_utils import logger
import uuid
import ast
from datasets import Dataset
from langchain_community.vectorstores import Chroma

from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_recall,
    context_precision,
    answer_correctness,
    answer_similarity,
)

METRIC_MAP = {
    "faithfulness": faithfulness,
    "answer_relevancy": answer_relevancy,
    "context_recall": context_recall,
    "context_precision": context_precision,
    "answer_correctness": answer_correctness,
    "answer_similarity": answer_similarity,
}


class QAPairDatasetGenerator:
    """Class to generate a QA pair dataset for evaluation of the application.
    The dataset is generated by prompting a LLM."""

    def __init__(self, vector_store: Chroma, llm: LlamaCpp):
        self.vector_store = vector_store
        self.llm = llm
        self.response_schema = qa_response_schema
        self.prompt_template = qa_generation_prompt
        self.parser = self.create_parser()
        self.prompt = self.create_prompt()
        self.dataset = None

    def create_parser(self) -> StructuredOutputParser:
        """Create a parser for the response schema."""
        return StructuredOutputParser.from_response_schemas(self.response_schema)

    def create_prompt(self) -> PromptTemplate:
        """Creates a prompt from the prompt template and the response schema."""
        format_instructions = self.parser.get_format_instructions()
        return PromptTemplate.from_template(
            self.prompt_template,
            partial_variables={"schema": format_instructions},
        )

    def create_dataset(
        self, save_dataset: bool = False, sets: int = 50, save_path: bool = None
    ) -> pd.DataFrame:
        """
        Create a dataset for evaluation.

        Args:
            save_dataset (bool): Whether to save the dataset to a file. Default is False.
            sets (int): The number of sets to include in the dataset. Default is 50.
            save_path (bool): The path to save the dataset file. If None, the dataset will not be saved.

        Returns:
            pandas DataFrame: The created dataset.

        """
        document_list = self.create_document_list()
        self.dataset = self.create_evaluation_data(
            document_list=document_list, sets=sets
        )
        self.save_dataset(save_path) if save_dataset else None
        return self.dataset

    class Evaluation:
        def create_document_list(self) -> list:
            """
            Create a list of all documents in the vector store with their corresponding IDs.

            Returns:
                A list of tuples, where each tuple contains an ID and a document.
            """
            documents = self.vector_store.get()["documents"]
            ids = self.vector_store.get()["ids"]
            document_list = list(zip(ids, documents))
            return document_list

    def create_evaluation_data(
        self, document_list: list, sets: int = 50
    ) -> pd.DataFrame:
        """
        Create evaluation data by generating question-answer pairs based on the given document list.

        Args:
            document_list (list): A list of documents to generate question-answer pairs from.
            sets (int, optional): The number of question-answer pairs to generate. Defaults to 50.

        Returns:
            pd.DataFrame: A DataFrame containing the generated question-answer pairs.
        """
        logger.info(f"Generating {sets} QA couples.".upper())

        chain = self.prompt | self.llm
        sampled_documents = random.sample(document_list, sets)

        # Generates list of dicts with context, context_id, question, and answer
        outputs = [
            self.generate_qa_couple(chain, context_id, sampled_context)
            for context_id, sampled_context in tqdm(
                sampled_documents, desc="Generating QA couples"
            )
        ]
        return pd.DataFrame(outputs)

    def generate_qa_couple(self, chain, context_id, sampled_context) -> dict:
        """
        Generate a question-answer couple with an LLM based on the given chain, context ID, and sampled context.

        Args:
            chain (Chain): The chain used for generating the question-answer couple.
            context_id (int): The ID of the context.
            sampled_context (str): The sampled context.

        Returns:
            dict: A dictionary containing the generated question-answer couple with the following keys:
                - 'context': The sampled context.
                - 'context_id': The ID of the context.
                - 'question': The generated question.
                - 'answer': The generated answer.
        """
        output_QA_couple = chain.invoke({"context": sampled_context})
        output_QA_couple = (
            json.loads(output_QA_couple)
            if isinstance(output_QA_couple, str)
            else output_QA_couple
        )
        output_QA_couple = {k.lower(): v for k, v in output_QA_couple.items()}

        answer_key = "antwort" if "antwort" in output_QA_couple else "answer"
        question_key = "frage" if "frage" in output_QA_couple else "question"

        return {
            "context": sampled_context,
            "context_id": context_id,
            "question": output_QA_couple[question_key],
            "answer": output_QA_couple[answer_key],
        }

    def save_dataset(self, save_path: bool = None) -> None:
        """
        Save the dataset as a CSV file.

        Args:
            save_path (str, optional): The path where the CSV file should be saved. If not provided, the file will be saved in the current directory.
        """
        date = datetime.datetime.now().date().isoformat()
        csv_name = f"generated_qa_pairs_{date}.csv"
        path = f"{save_path}/{csv_name}" if save_path else csv_name
        self.dataset.to_csv(path, index=False)
        logger.info(f"Dataset saved as {csv_name}".upper())


class LLMAnswerGenerator:
    """Generates answers to a given set of questions using a LLM chain."""

    def __init__(self, chain, qa_pair_dataset: pd.DataFrame = None, judge=None):
        self.chain = chain
        self.judge = judge
        self.qa_pair_dataset = qa_pair_dataset
        self.eval_dataset: Dataset = None

    def execute_chain(self, question: str) -> dict:
        """
        Generates the LLM output to a given question.

        Args:
            question (str): The question to be processed.

        Returns:
            The output of the chain execution, or the result of the judge's evaluation if a judge is present.
        """
        self.chain.question_id = uuid.uuid4().hex
        chain_output = self.chain.execute(question)
        return self.judge.execute(chain_output) if self.judge else chain_output

    def generate_contexts(self, llm_outputs: dict) -> list:
        """
        Extracts the contexts based on the given LLM outputs.
        LLM outputs differ from each other depending on the chain used.

        Args:
            llm_outputs (list): A list of LLM outputs.

        Returns:
            list: A list of generated contexts.

        """
        if self.judge:
            return [output["context"] for output in llm_outputs]
        else:
            return [
                [item["page_content"] for item in output["context"]]
                for output in llm_outputs
            ]

    def generate_outputs(self) -> Dataset:
        """
        Generates outputs for evaluation. The returned dataset has to be of class Dataset, for later evaluation.

        Returns:
            eval_dataset (Dataset): The evaluation dataset containing the generated outputs.
        """
        if self.judge:
            self.judge.label = "evaluation"

        questions = self.qa_pair_dataset["questions"].tolist()
        ground_truths = self.qa_pair_dataset["ground_truth"].tolist()
        llm_outputs = [
            self.execute_chain(question)
            for question in tqdm(questions, desc="Generating LLM answers")
        ]

        contexts = self.generate_contexts(llm_outputs)
        # Dict of lists for each key.
        data = {
            "question": questions,
            "answer": [output["answer"] for output in llm_outputs],
            "contexts": contexts,
            "ground_truth": ground_truths,
        }

        if self.judge:
            data["correctness_score"] = [
                output.get("correctness", None) for output in llm_outputs
            ]
            data["send_to_expert"] = [
                (
                    1
                    if output.get("correctness", None) is None
                    or output["correctness"] < 3
                    else 0
                )
                for output in llm_outputs
            ]

        self.eval_dataset = Dataset.from_dict(data)

        return self.eval_dataset

    def save_dataset(self, save_path: str = None):
        """Saves the generated answers as a CSV file."""
        csv_name = "answers_judge.csv" if self.judge else "answers_doc.csv"
        path = f"{save_path}/{csv_name}" if save_path else csv_name
        self.eval_dataset.to_csv(path, index=False)
        logger.info(f"LLM ANSWERS SAVED AS {csv_name}")


class Evaluator:
    """Evaluator class to evaluate the LLM outputs on RAGAS metrics."""

    def __init__(self, llm: LlamaCpp, embedding_model: HuggingFaceEmbeddings):
        self.llm = llm
        self.embedding_model = embedding_model
        self.eval_dataset = None
        self.result_data = None

    def convert_to_list(self, column: pd.Series) -> pd.Series:
        """Converts a column of the evaluation dataset to a list of strings."""
        self.eval_dataset[column] = self.eval_dataset[column].apply(ast.literal_eval)

    def load_from_csv(self, path: str) -> pd.DataFrame:
        """Loads the evaluation dataset from a CSV file and returns it as a pands DataFrame."""
        self.eval_dataset = pd.read_csv(path)
        self.convert_to_list("contexts")
        self.eval_dataset = Dataset.from_pandas(self.eval_dataset)
        return self.eval_dataset

    def select_metrics(self, metrics: list) -> list:
        """Selects the metrics to evaluate the LLM outputs on.

        Returns:
            list: A list of selected metrics functions from RAGAS."""
        if isinstance(metrics, str):
            metrics = [metrics]

        selected_metrics = []
        for metric in metrics:
            if metric in METRIC_MAP:
                selected_metrics.append(METRIC_MAP[metric])
            else:
                raise ValueError(f"Unknown metric: {metric}")

        return selected_metrics

    def evaluate_dataset(self, metrics: list) -> pd.DataFrame:
        """
        Evaluate the dataset using the specified metrics from RAGAS libary.

        Args:
            metrics (list): A list of metrics to be used for evaluation.

        Returns:
            pd.DataFrame: The evaluation results as a pandas DataFrame.
        """
        selected_metrics = self.select_metrics(metrics)
        self.result_data = evaluate(
            dataset=self.eval_dataset,
            metrics=selected_metrics,
            llm=self.llm,
            embeddings=self.embedding_model,
        )
        return self.result_data.to_pandas()

    def load_evaluate_dataset(self, file_path: str, metrics: list) -> pd.DataFrame:
        """Load the dataset from a CSV file and evaluate it using the specified metrics."""
        self.load_from_csv(file_path)
        return self.evaluate_dataset(metrics)

    def save_dataset(self, dataset_name: str, save_path: str = None):
        """Save the evaluation results as a CSV file."""
        csv_name = f"results_{dataset_name}.csv"
        path = f"{save_path}/{csv_name}" if save_path else csv_name
        self.result_data.to_pandas().to_csv(path, index=False)
        logger.info(f"EVALUATION SAVED {csv_name}")


class BingPromptGenerator:
    """Gerates prompts for Bing Chat evaluation."""

    def __init__(self):
        self.prompt_template = bing_chat_template
        self.response_schema = bing_chat_response_schema
        self.prompt = self.create_prompt()

    def create_prompts(self, qa_pair_dataset):
        """Creates a list of prompts for Bing Chat evaluation."""
        prompts = []
        for _, row in qa_pair_dataset.iterrows():
            input_variables = {"context": row["contexts"], "question": row["questions"]}
            prompts.append(self.prompt.format(**input_variables))
        return prompts

    def create_prompt(self):
        """Creates a prompt from the prompt template and the response schema."""
        parser = StructuredOutputParser.from_response_schemas(self.response_schema)
        format_instructions = parser.get_format_instructions()
        return PromptTemplate.from_template(
            self.prompt_template,
            partial_variables={"schema": format_instructions},
        )
